#  Agente RL-ARIMA para Forecasting de Series Temporales

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-Ready-green.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.27+-red.svg)](https://streamlit.io/)

##  Descripci贸n

Sistema completo de **Aprendizaje Reforzado (RL)** para la optimizaci贸n autom谩tica de hiperpar谩metros de modelos **ARIMA** aplicado al pron贸stico de consumo el茅ctrico. El proyecto implementa un agente **DQN** (Deep Q-Network) que aprende a seleccionar configuraciones 贸ptimas (p, d, q) mediante interacci贸n con un entorno **Gymnasium** personalizado.

###  Caracter铆sticas Principales

- ?**Agente RL entrenado** con 50k timesteps usando Stable-Baselines3
- ?**Entorno Gymnasium personalizado** con funci贸n de recompensa multiobjetivo
- ?**Interfaz web interactiva** con Streamlit (dual mode: autom谩tico/manual)
- ?**Comparaci贸n de m煤ltiples modelos** ARIMA con m茅tricas completas
- ?**Diagn贸stico de residuos** completo (ACF, Q-Q Plot, Ljung-Box, Jarque-Bera)
- ?**Containerizaci贸n Docker** con generaci贸n autom谩tica de datos
- ?**Dataset real**: 60 meses de consumo el茅ctrico alem谩n (OPSD)

---

##  Fundamento T茅cnico

### Problema a Resolver

La selecci贸n manual de hiperpar谩metros ARIMA (p, d, q) es:
- ?**Consume tiempo**: Requiere 60+ iteraciones t铆picamente
-  **Requiere expertise**: An谩lisis de ACF/PACF y pruebas de estacionariedad
-  **Proceso iterativo**: Ajuste basado en AIC, BIC, RMSE

### Soluci贸n Propuesta

Usar un **agente de aprendizaje reforzado** que:
1. **Aprende pol铆ticas 贸ptimas** mediante exploraci贸n del espacio de configuraciones
2. **Reduce tiempo en 50-70%** vs. grid search exhaustivo
3. **Generaliza** a nuevas series temporales con fine-tuning m铆nimo

### Arquitectura del Sistema

```
??                 INTERFAZ WEB (Streamlit)               ?? ? ? ??? ?Exploraci贸n ? ? Agente RL/  ? ? Comparaci贸n  ??? ?   Datos    ? ?   Manual    ? ?   Modelos    ??? ? ? ???                            ???             AGENTE RL (DQN - Stable-Baselines3)        ?? ?Red neuronal: [128, 128]                             ?? ?Exploration: 蔚-greedy (1.0 ?0.05)                   ?? ?Replay buffer: 10,000 experiencias                   ??                            ???        ENTORNO GYMNASIUM (ARIMAHyperparamEnv)          ?? ?Estados: [RMSE, AIC, p, d, q, step, ...]            ?? ?Acciones: (p, d, q) discreto                         ?? ?Recompensa: f(accuracy, AIC, time, diagnostics)      ??                            ???             MODELOS ARIMA (Statsmodels)                ?? ?Entrenamiento en 48 meses                            ?? ?Validaci贸n en 6 meses                                ?? ?Prueba en 6 meses                                    ??```

---

##  Instalaci贸n R谩pida

### Opci贸n 1: Docker (Recomendado) 猸?
```bash
# 1. Descomprimir
unzip arima-rl-project.zip
cd arima-rl-project

# 2. Dar permisos
chmod +x scripts/*.sh

# 3. Construir y ejecutar
./scripts/build_docker.sh build
./scripts/build_docker.sh run

# 4. Acceder
# http://localhost:8501
```

**Tiempo de instalaci贸n**: 5-10 minutos  
**Espacio requerido**: ~2 GB

### Opci贸n 2: Instalaci贸n Local

```bash
# 1. Descomprimir
unzip arima-rl-project.zip
cd arima-rl-project

# 2. Crear entorno virtual
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Generar datos
python data/download_data.py

# 5. Ejecutar aplicaci贸n
streamlit run src/app.py
```

---

##  Uso del Sistema

### 1锔 Exploraci贸n de Datos (Tab 1)

-  Visualizaci贸n de serie temporal completa
-  Estad铆sticas descriptivas
-  Prueba de estacionariedad (ADF)
-  Funciones ACF/PACF interactivas

### 2锔 Agente RL / Modo Manual (Tab 2)

####  Modo Autom谩tico (Agente RL)

1. Clic en **" Predecir Mejor Configuraci贸n"**
2. El agente analiza la serie y propone (p, d, q) 贸ptimo
3. Clic en **"讹 Entrenar Modelo ARIMA Propuesto"**
4. Ver m茅tricas (AIC, BIC, RMSE, MAE) y pron贸stico

> 锔 **Nota**: Requiere modelo RL entrenado (ver secci贸n **Entrenamiento**)

#### 锔?Modo Manual (Sliders)

1. Ajustar sliders p, d, q manualmente
2. Clic en **" Entrenar y Evaluar Modelo"**
3. Explorar diferentes configuraciones

### 3锔 Comparaci贸n de Modelos (Tab 3)

1. Configurar hasta 3 modelos ARIMA diferentes
2. Clic en **" Comparar Modelos"**
3. Ver tabla ordenada por AIC (mejor resaltado)
4. Exportar resultados a CSV

### 4锔 Diagn贸stico de Residuos (Tab 4)

-  Gr谩ficas de diagn贸stico (residuos, histograma, Q-Q, ACF)
-  Estad铆sticas (media, desv. est谩ndar, tests)
- ?Verificaci贸n de supuestos ARIMA

---

##  Entrenamiento del Agente RL

El agente RL **NO** se entrena autom谩ticamente durante el build Docker (para reducir tiempo). Entrenar despu茅s de iniciar el sistema:

### Dentro de Docker

```bash
# Acceder al contenedor
docker exec -it arima-rl-container bash

# Entrenar
python -m src.rl_agent --train --timesteps 50000

# Salir
exit
```

### Instalaci贸n Local

```bash
# Opci贸n A: Con script
chmod +x scripts/train_agent.sh
./scripts/train_agent.sh

# Opci贸n B: Comando directo
python -m src.rl_agent --train --timesteps 50000

# Opci贸n C: Entrenamiento r谩pido (pruebas)
python -m src.rl_agent --train --timesteps 10000
```

**Tiempos de entrenamiento**:
- 10k timesteps: ~5-10 minutos
- 50k timesteps: ~30-60 minutos

**Modelo guardado**: `models/arima_dqn_agent.zip`

---

##  Dataset: Consumo El茅ctrico Alem谩n

### Fuente de Datos

- **Origen**: [Open Power System Data (OPSD)](https://open-power-system-data.org/)
- **Per铆odo**: 2013-2017 (60 meses)
- **Frecuencia**: Mensual (agregado desde datos horarios)
- **Unidades**: GWh (Gigawatt-hora)

### Divisi贸n de Datos

| Conjunto   | Meses | Porcentaje | Uso                          |
|------------|-------|------------|------------------------------|
| Train      | 48    | 80%        | Entrenamiento ARIMA y RL     |
| Validation | 6     | 10%        | Selecci贸n de hiperpar谩metros |
| Test       | 6     | 10%        | Evaluaci贸n final             |

### Generaci贸n Autom谩tica

El script `data/download_data.py`:
1. ?Intenta descargar datos reales de OPSD
2. ?Si falla, genera datos sint茅ticos realistas
3. ?Convierte a frecuencia mensual
4. ?Divide en train/val/test
5. ?Guarda CSVs individuales

---

## 锔 Configuraci贸n Avanzada

### Archivo `config/config.yaml`

```yaml
# Agente RL
rl_agent:
  total_timesteps: 50000
  learning_rate: 0.0001
  buffer_size: 10000
  exploration_fraction: 0.3

# Entorno
environment:
  p_max: 5
  d_max: 2
  q_max: 4
  max_steps: 50
  reward_weights:
    accuracy: 1.0
    aic: 0.3
    time: 0.1
    diagnostics: 0.2

# ARIMA
arima:
  confidence_level: 0.95
  max_training_time: 30
```

---

##  M茅tricas de Evaluaci贸n

### Calidad del Pron贸stico

- **RMSE** (Root Mean Squared Error): Error cuadr谩tico medio
- **MAE** (Mean Absolute Error): Error absoluto medio
- **MAPE** (Mean Absolute Percentage Error): Error porcentual
- **R虏**: Coeficiente de determinaci贸n

### Selecci贸n de Modelo

- **AIC** (Akaike Information Criterion): Balance ajuste/complejidad
- **BIC** (Bayesian Information Criterion): Penaliza m谩s la complejidad
- **AICc**: AIC corregido para muestras peque帽as

### Diagn贸stico de Residuos

- **Test de Normalidad**: Jarque-Bera (p > 0.05)
- **Test de Autocorrelaci贸n**: Ljung-Box (p > 0.05)
- **Homocedasticidad**: Ratio de varianzas < 2.0
- **Media de residuos**: ?0

---

##  Comandos tiles

### Docker

```bash
# Ver logs en tiempo real
docker logs -f arima-rl-container

# Detener contenedor
./scripts/build_docker.sh stop

# Limpiar todo (contenedor + imagen)
./scripts/build_docker.sh clean

# Estado del sistema
./scripts/build_docker.sh status

# Abrir shell
./scripts/build_docker.sh shell
```

### TensorBoard (Monitoreo de Entrenamiento)

```bash
tensorboard --logdir models/tensorboard_logs
# Acceder a http://localhost:6006
```

### Evaluar Agente Entrenado

```bash
python -m src.rl_agent --eval --model-path models/arima_dqn_agent.zip
```

---

##  Estructura del Proyecto

```
arima-rl-project/
 README.md                  # Este archivo
 QUICKSTART.md             # Gu铆a de inicio r谩pido
 Dockerfile                # Containerizaci贸n completa
 requirements.txt          # Dependencias Python
 .dockerignore            # Archivos excluidos de Docker
 .gitignore               # Archivos excluidos de Git
? data/
?   download_data.py     # Script de descarga/generaci贸n de datos
?   germany_monthly_power.csv    # 60 meses completos
?   train.csv            # 48 meses
?   validation.csv       # 6 meses
?   test.csv             # 6 meses
?   metadata.txt         # Informaci贸n del dataset
? src/
?   __init__.py          # Inicializaci贸n del paquete
?   data_processor.py    # Procesamiento de series temporales
?   arima_env.py         # Entorno Gymnasium personalizado
?   rl_agent.py          # Agente DQN (Stable-Baselines3)
?   arima_utils.py       # Utilidades ARIMA
?   app.py               # Interfaz web Streamlit
? scripts/
?   build_docker.sh      # Construcci贸n/ejecuci贸n Docker
?   run_app.sh           # Ejecuci贸n de aplicaci贸n
?   train_agent.sh       # Entrenamiento del agente RL
? config/
?   config.yaml          # Configuraci贸n completa
? assets/
?   style.css            # Estilos CSS personalizados
?   custom.js            # JavaScript personalizado
?   logs/                # Logs de aplicaci贸n
? models/                  # Modelos entrenados
     arima_dqn_agent.zip  # Modelo RL principal
     tensorboard_logs/    # Logs de TensorBoard
```

---

##  Resultados Esperados

### Agente RL vs. Grid Search

| M茅todo       | Configuraciones | Tiempo | AIC ptimo | Convergencia |
|--------------|-----------------|--------|------------|--------------|
| Grid Search  | 60              | 100%   | Garantizado| N/A          |
| Agente RL    | ~30-40          | 30-50% | 95-98%     | 30k steps    |

### Mejoras Reportadas en Literatura

- **Hyp-RL (2019)**: RL supera optimizaci贸n bayesiana con 50 datasets
- **ARIMA-LSTM (2023)**: Mejoras de 13% en MAE sobre modelos individuales
- **RLMC (AAAI 2022)**: Combinaci贸n din谩mica de modelos con RL

---

##  Soluci贸n de Problemas

### Problema: Datos no encontrados

```bash
python data/download_data.py
```

### Problema: Modelo RL no encontrado

```bash
python -m src.rl_agent --train --timesteps 10000
```

### Problema: Puerto 8501 ocupado

```bash
# Cambiar puerto
PORT=8502 ./scripts/run_app.sh

# O liberar puerto (Linux/Mac)
lsof -ti:8501 | xargs kill -9
```

### Problema: Error de memoria durante entrenamiento

```yaml
# Editar config/config.yaml
rl_agent:
  buffer_size: 5000  # Reducir de 10000
```

---

##  Referencias

### Papers Principales

1. **Hyp-RL**: Jomaa et al. (2019) - Hyperparameter Optimization by RL
2. **RLMC**: Fu et al. (2022) - RL Based Dynamic Model Combination
3. **ARIMA-LSTM**: Wang & Li (2023) - Peak Electrical Energy Consumption Prediction

### Documentaci贸n T茅cnica

- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)
- [Statsmodels ARIMA](https://www.statsmodels.org/stable/)
- [Streamlit](https://docs.streamlit.io/)
- [Gymnasium](https://gymnasium.farama.org/)

### Datos

- [Open Power System Data](https://open-power-system-data.org/)
- [PyPSA](https://pypsa.org/)

---

##  Contribuciones

Este proyecto es parte de un reporte t茅cnico acad茅mico sobre "Agentificaci贸n de Modelos ARIMA con Aprendizaje Reforzado". Consulte el reporte PDF completo para fundamentos matem谩ticos detallados.

---

##  Licencia

Este proyecto se distribuye bajo licencia MIT. Ver archivo `LICENSE` para m谩s detalles.

---

##  Soporte

Para preguntas o problemas:
1. Consulte `QUICKSTART.md` para gu铆a r谩pida
2. Revise las instrucciones detalladas en `INSTRUCCIONES_DESPLIEGUE.txt`
3. Consulte el reporte t茅cnico PDF para fundamentos

---

## ?Caracter铆sticas Futuras (Roadmap)

- [ ] Soporte para SARIMA (estacionalidad)
- [ ] Variables ex贸genas (ARIMAX/SARIMAX)
- [ ] Ensemble RL-ARIMA-LSTM
- [ ] Meta-learning cross-dataset
- [ ] Optimizaci贸n multi-objetivo (NSGA-II)
- [ ] API REST para integraci贸n
- [ ] Dashboard de monitoreo en tiempo real

---

**Desarrollado con わ para optimizaci贸n autom谩tica de series temporales mediante Aprendizaje Reforzado**
